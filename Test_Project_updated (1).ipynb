{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset identification: classification and information retrieval (document relevance) approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "\n",
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import glob\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Restructuring data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting publication-dataset pairs from \"data_set_citations.json\"\n",
    "\n",
    "# Importing and loading the JSON file\n",
    "\n",
    "file = open('data_set_citations.json', 'r')\n",
    "json_file = json.load(file)\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/2835559/parsing-values-from-a-json-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists and extracting publication and dataset indices\n",
    "\n",
    "list_pub = []\n",
    "list_dataset = []\n",
    "for i in json_file:\n",
    "    pub = i['publication_id']\n",
    "    dataset = i['data_set_id']\n",
    "    list_pub.append(pub)\n",
    "    list_dataset.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data to a dataframe and then to a .csv file\n",
    "\n",
    "data = pandas.DataFrame(list_pub, columns=['publication'])\n",
    "data['dataset']=list_dataset\n",
    "#data.to_csv(\"Pub_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders on the local machine based on dataset indexes\n",
    "\n",
    "set_dataset = set(list_dataset)\n",
    "for i in set_dataset:\n",
    "    os.mkdir(str(i))\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/17889814/create-folders-in-directory-from-values-in-list-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving of files from \"text\" folder to multiple folders with dataset indices is performed using \"Pub_dataset.csv\" and a VBA code in Excel per this reference link: https://www.ozgrid.com/forum/forum/help-forums/excel-general/126154-vba-code-to-move-multiple-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have the following data structure: a folder which contains multiple folders with dataset indices (the classification \"labels\") and those folders contain texts of all articles associated with a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading restuctured data using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn load_files function, importing restructured data\n",
    "# Access to text files is through .data and to classification labels is through .target (folder names based on dataset indices)\n",
    "\n",
    "data_folder = 'sample_2/'\n",
    "all_data = load_files(data_folder)\n",
    "\n",
    "# Reference link: https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test data are created using sklearn train_test_split function\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    all_data.data, all_data.target, test_size=0.5)\n",
    "\n",
    "# Reference link: https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performing TF-IDF transformation on data, training classifiers through Pipeline from Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_one = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf', MultinomialNB()),\n",
    "... ])\n",
    "\n",
    "# Reference link: https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_one.fit(docs_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier_two = Pipeline([('vect', CountVectorizer()),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "                           ('clf', SGDClassifier(penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_two.fit(docs_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_one = classifier_one.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba = classifier_one.predict_proba(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_two = classifier_two.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_confidence_score = classifier_two.decision_function(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_one = numpy.mean(predicted_one == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_two = numpy.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect = CountVectorizer()\n",
    "#X_train_counts = count_vect.fit_transform(docs_train)\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Reference link: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial_classifier = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "#linear_SVC_classifier = LinearSVC(random_state=0).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/15015710/how-can-i-know-probability-of-class-predicted-by-predict-function-in-support-v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without probabilities\n",
    "\n",
    "#predicted_multinomial = multinomial_classifier.predict(docs_test)\n",
    "#predicted_linear_SVC = linear_SVC_classifier.predict(docs_test)\n",
    "\n",
    "# With probabilities\n",
    "\n",
    "#predicted_multinomial_prob = multinomial_classifier.predict_proba(docs_test)\n",
    "#predicted_linear_SVC_prob = linear_SVC_classifier.predict_proba(docs_test)\n",
    "\n",
    "# Computing confidence scores\n",
    "\n",
    "#predicted_multinomial_confidence = multinomial_classifier.decision_function(docs_test)\n",
    "#predicted_linear_SVC_confidence = linear_SVC_classifier.decision_function(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes classifier results\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted_multinomial,\n",
    "#        target_names=docs_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC classifier results\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted_linear_SVC,\n",
    "#        target_names=docs_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Multinomial Naive Bayes\n",
    "\n",
    "#metrics.confusion_matrix(y_test, predicted_multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Linear SVC\n",
    "\n",
    "#metrics.confusion_matrix(y_test, predicted_linear_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. INFORMATION RETRIEVAL (DOCUMENT RELEVANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting relevant fields from JSON file on datasets (name, metadata description and list of mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data_sets.json file and extracting 'title', 'description' and 'mention_list' fields\n",
    "\n",
    "file = open('data_sets.json', 'r')\n",
    "json_file = json.load(file)\n",
    "\n",
    "list_title = []\n",
    "list_description = []\n",
    "list_mention = []\n",
    "for i in json_file:\n",
    "    title = i['title']\n",
    "    description = i['description']\n",
    "    mention = i['mention_list']\n",
    "    list_title.append(title)\n",
    "    list_description.append(description)\n",
    "    list_mention.append(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a string\n",
    "\n",
    "list_mention_updated = []\n",
    "for i in list_mention:\n",
    "    string = ' '.join(i)\n",
    "    list_mention_updated.append(string)\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/5618878/how-to-convert-list-to-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Concatenating all information on datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_list= [' '.join(i) for i in zip(list_title, list_description, list_mention_updated)]\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/40912968/how-to-concatenate-multiple-lists-element-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating corresponding text files for further download in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dataset_list = []\n",
    "for i in set_dataset:\n",
    "    y = str(i) + '.txt'\n",
    "    set_dataset_list.append(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y in zip(created_list, set_dataset_list):\n",
    "    with open(y, 'w', encoding='utf-8') as output:\n",
    "        output.write(i)\n",
    "        \n",
    "# Reference link: https://stackoverflow.com/questions/6673092/printing-out-elements-of-list-into-separate-text-files-in-python\n",
    "# Reference link: https://stackoverflow.com/questions/27092833/unicodeencodeerror-charmap-codec-cant-encode-characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(os.path.join(os.getcwd(), 'sample_1', '*.txt'))\n",
    "\n",
    "list_text_files = []\n",
    "\n",
    "for i in text_files:\n",
    "    with open(i) as y:\n",
    "        list_text_files.append(y.read())\n",
    "        \n",
    "# Reference link: https://stackoverflow.com/questions/42407976/loading-multiple-text-files-from-a-folder-into-a-python-list-variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating TD-IDF representations of dataset strings and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect = CountVectorizer()\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#counts = count_vect.fit_transform(data_files)\n",
    "#tf_idf = tfidf_transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts_documents = count_vect.fit_transform(all_data.data)\n",
    "#tf_idf_documents = tfidf_transformer.fit_transform(counts_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = list_text_files + all_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(combined_list)\n",
    "(tfidf * tfidf.T).A\n",
    "\n",
    "# Reference link: https://stackoverflow.com/questions/8897593/similarity-between-two-text-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also to try using gensim library\n",
    "# Reference link: https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
